import os
import json
import logging
import time
from concurrent.futures import ThreadPoolExecutor
from uuid import uuid4
from dotenv import load_dotenv
from redis import Redis
from tenacity import retry, stop_after_attempt, wait_fixed
from langchain_google_community import GoogleDriveLoader
from langchain_ollama import OllamaEmbeddings
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
import schedule

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

load_dotenv()

MODEL = os.getenv("RAG_MODEL", "cwchang/llama3-taide-lx-8b-chat-alpha1:q3_k_s")
CHROMA_PATH = os.getenv("CHROMA_PATH", "./fortunetell_chroma__db")
SERVICE_ACCOUNT_PATH = os.getenv("SERVICE_ACCOUNT_PATH", "service_account.json")
FOLDER_ID = os.getenv("FOLDER_ID")
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")
SYNC_INTERVAL_SECONDS = int(os.getenv("SYNC_INTERVAL_SECONDS", "86400")) 

# åˆå§‹åŒ– Redis
redis_client = Redis.from_url(REDIS_URL, decode_responses=True)
EMBEDDED_KEY = "embedded_file_ids"
LAST_SYNC_KEY = "last_sync_time"
SYNC_STATUS_KEY = "sync_status"

# å…¨å±€ db
embeddings = OllamaEmbeddings(model=MODEL)
db = Chroma(
    collection_name="fortunetelling_rag_db",
    embedding_function=embeddings,
    persist_directory=CHROMA_PATH,
)

def load_embedded_record():
    """å¾ Redis è¼‰å…¥å·²åµŒå…¥ ID set"""
    return set(redis_client.smembers(EMBEDDED_KEY))

def save_embedded_record(new_ids: set):
    """æ–°å¢ ID åˆ° Redis set"""
    if new_ids:
        redis_client.sadd(EMBEDDED_KEY, *new_ids)
        logger.info(f"Added {len(new_ids)} IDs to embedded record")

def clean_obsolete_embeddings(current_ids: set):
    """åˆªé™¤å·²ç§»é™¤çš„æ–‡ä»¶åµŒå…¥"""
    embedded_ids = load_embedded_record()
    obsolete = embedded_ids - current_ids
    if obsolete:
        for file_id in obsolete:
            results = db.get(where={"id": file_id})
            if results:
                db.delete(results["ids"])
                redis_client.srem(EMBEDDED_KEY, file_id)
                logger.info(f"Removed obsolete embedding for ID: {file_id}")

@retry(stop=stop_after_attempt(3), wait=wait_fixed(300))  # é‡è©¦ 3 æ¬¡ï¼Œé–“éš” 5 åˆ†é˜
def sync_drive_embeddings():
    logger.info("ğŸ”„ é–‹å§‹åŒæ­¥ Google Drive æ–‡ä»¶...")
    try:
        redis_client.set(LAST_SYNC_KEY, int(time.time()))  # è¨˜éŒ„é–‹å§‹æ™‚é–“
        redis_client.set(SYNC_STATUS_KEY, "running")

        embedded_ids = load_embedded_record()

        loader = GoogleDriveLoader.from_service_account_file(
            service_account_path=SERVICE_ACCOUNT_PATH,
            folder_id=FOLDER_ID,
            recursive=False,
        )
        docs = loader.load()

        current_ids = {doc.metadata.get("id") for doc in docs}
        clean_obsolete_embeddings(current_ids)

        new_docs = [doc for doc in docs if doc.metadata.get("id") not in embedded_ids]
        logger.info(f"âœ… æ–°å¢æ–‡ä»¶æ•¸é‡ï¼š{len(new_docs)}")

        if not new_docs:
            logger.info("âœ… ç„¡éœ€æ›´æ–°ï¼Œæ‰€æœ‰æ–‡ä»¶å·²åµŒå…¥éã€‚")
            redis_client.set(SYNC_STATUS_KEY, "success")
            return

        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        split_docs = splitter.split_documents(new_docs)

        def embed_chunk(chunk):
            uuids = [str(uuid4()) for _ in chunk]
            db.add_documents(chunk, ids=uuids)

        with ThreadPoolExecutor(max_workers=2) as executor:  # Fly.io è³‡æºé™åˆ¶ï¼Œé™ç‚º 2 åŸ·è¡Œç·’
            chunk_size = max(1, len(split_docs) // 2)
            chunks = [split_docs[i:i + chunk_size] for i in range(0, len(split_docs), chunk_size)]
            executor.map(embed_chunk, chunks)

        logger.info(f"âœ… å·²åµŒå…¥ {len(split_docs)} å€å¡Š")

        new_ids = {doc.metadata.get("id") for doc in new_docs}
        save_embedded_record(new_ids)
        redis_client.set(SYNC_STATUS_KEY, "success")
        logger.info("âœ… åµŒå…¥ç´€éŒ„å·²æ›´æ–°")
    except Exception as e:
        redis_client.set(SYNC_STATUS_KEY, f"failed: {str(e)}")
        logger.error(f"Sync error: {str(e)}")
        raise

def validate_texts(texts: list[str]) -> bool:
    """é©—è­‰æ‰‹å‹•è¼¸å…¥æ–‡æœ¬"""
    if not texts:
        logger.warning("Empty texts received")
        return False
    for text in texts:
        if len(text) > 10000:
            logger.warning("Text too long: %d", len(text))
            return False
    return True

def embedd_manual_from_text(texts: list[str], store: bool = True):
    if not validate_texts(texts):
        logger.error("Invalid texts, skipping embed")
        return None

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = splitter.create_documents(texts)
    logger.info(f"âœ… æ‰‹å‹•å‚³å…¥æ–‡å­—ï¼Œå…±åˆ‡åˆ†ç‚º {len(split_docs)} å€å¡Š")

    if store:
        def embed_chunk(chunk):
            uuids = [str(uuid4()) for _ in chunk]
            db.add_documents(chunk, ids=uuids)

        with ThreadPoolExecutor(max_workers=2) as executor:
            chunk_size = max(1, len(split_docs) // 2)
            chunks = [split_docs[i:i + chunk_size] for i in range(0, len(split_docs), chunk_size)]
            executor.map(embed_chunk, chunks)
        logger.info("âœ… æ‰‹å‹•åµŒå…¥è³‡æ–™å·²å„²å­˜")
    else:
        return db

def load_vector_db():
    """è¼‰å…¥ dbï¼ˆå·²å…¨å±€åˆå§‹åŒ–ï¼‰"""
    return db

def run_scheduler():
    """é‹è¡Œå®šæ™‚ä»»å‹™"""
    schedule.every(SYNC_INTERVAL_SECONDS).seconds.do(sync_drive_embeddings)
    logger.info(f"ğŸ“… å®šæ™‚ä»»å‹™å·²å•Ÿå‹•ï¼Œæ¯ {SYNC_INTERVAL_SECONDS} ç§’æª¢æŸ¥ä¸€æ¬¡ Google Drive")
    while True:
        schedule.run_pending()
        time.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥ï¼Œé¿å… CPU éè¼‰

if __name__ == "__main__":
    import threading
    # åœ¨å¾Œå°åŸ·è¡Œç·’é‹è¡Œå®šæ™‚ä»»å‹™
    scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
    scheduler_thread.start()
    # ä¿æŒä¸»åŸ·è¡Œç·’å­˜æ´»
    try:
        while True:
            time.sleep(3600)  # ä¸»åŸ·è¡Œç·’ä¼‘çœ ï¼Œä»»å‹™ç”±å¾Œå°è™•ç†
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ åœæ­¢å®šæ™‚ä»»å‹™")